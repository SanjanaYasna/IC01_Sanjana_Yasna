{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Sanjana Yasna\n",
    "\n",
    "The Following is on an .ipynb file, but the r-code I use is pasted in with relevant outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 (p. 121 ISLR)\n",
    "\n",
    "Sales has no relationship with newspaper sales (null hypothesis accepted), but sales has a relationship with TV and radio advertising (null hypothesis rejected).\n",
    "\n",
    "We see that the t-statistic is relatively large in comparison to the standard error for both TV and radio to sales, meaning the slope and intercept of the regression line is likely not close to 0. \n",
    "This also explains the very low p-values (well below even a 1% cutoff) that indicate there is a very low chance that this relationship between the predictor and response in these two cases is due to chance. \n",
    "The odd one out is sales to newspaper, which has a very high p-value and a t statistic that isn't as many magnitudes larger than the standard error as the relationship with TV and sales, so it fits criteria for keeping hte null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 (p. 122 ISLR)\n",
    "\n",
    "## (a) \n",
    "\n",
    "RSS would be lower for cubic regression than linear regression\n",
    "\n",
    "Cubic regression aims to reduce the error term $\\epsilon$ and since the curve of fit has three fitted coefficients as opposed to the 1 for linear regression, it typically gets lower variance in its fit and better minimizes standard error of its reference points. So, RSS will eventually be lower in cubic since the model has more opportunity to overfit based on subtleties.\n",
    "\n",
    "## (b) \n",
    "\n",
    "Test set RSS would be lower in linear regression than cubic regression\n",
    "\n",
    "The underlying relationsihp in the data is linear, so RSS training on cubic regression has greater chance of overfit that would lead to poor test performance than linear regression\n",
    "\n",
    "## (c) \n",
    "\n",
    "Again, cubic regression would yield lower RSS than linear regression\n",
    "\n",
    "Similar reasoning to (a). If the data is very non-linear and follows a cubic or spline pattern, RSS may be much lower for cubic regression in comparison to linear regression\n",
    "\n",
    "## (d) \n",
    "\n",
    "Unknown, as is the underlying distribution is unknown, test performance can't be said\n",
    "\n",
    "Cubic regression may generally do better than linear regression, but if the test set is still mostly linear, linear regression would do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 (p. 123 ISLR) \n",
    "\n",
    "Linear regression line prediction at given point is: $\\hat{y_i} = \\hat{B_0} + \\hat{B_0}x_i$\n",
    "\n",
    "$\\hat{B_0} = \\bar{y}- \\hat{B_1}\\bar{x}$\n",
    "\n",
    "We want to minimize the error $e_i = y_i - \\hat{y}_i$, so aim for $y_i = \\hat{y}_i$ to try for zero arror and set the regression line equation to $y_i$ instead:\n",
    "\n",
    "$y_i =  \\hat{B_0} + \\hat{B_1}x_i$ And after substituting for $B_0$:\n",
    "\n",
    "$y_i = \\bar{y} - \\hat{B_1}\\bar{x} + \\hat{B_1}x_i$\n",
    "\n",
    "If we allow $x_i = \\bar{x}$ to cancel terms out we are left with just $y_i = \\bar{y}$ \n",
    "\n",
    "So if $x_i = \\bar{x}$ and $y_i = \\bar{y}$, the prediction $\\hat{y_i}$ will equal the true value $y_i$ at a certain point, minimizing RSS. \n",
    "\n",
    "$(\\bar{x}, \\bar{y})$ also makes sense in direct context of just minimizing the individual $ \\hat{B_0} $ and $\\hat{B_0}$ terms:\n",
    "\n",
    "$\\hat{B_1} = \\sum^{n}_{i=1}(x_i - \\bar{x})(y_i -\\bar{y}) /   \\sum^{n}_{i=1}(x_i -\\bar{x})^2$\n",
    "\n",
    "So when $x_i = \\bar{x}$ and $y_i = \\bar{y}$, $\\hat{B_1} = 0$ at that point, and $\\hat{B_0} = \\bar{y}$, which would push the prediction $\\hat{y_i} = \\bar{y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 (p. 123-124 ISLR)\n",
    "\n",
    "## (a)\n",
    "\n",
    "There is a clear negative relationship between horsepower and mpg that doesn't seem to be by chance, as the p-value is a mere 2.2e-16 and the F-statistic is 599.7, a strong indication we should reject the null hypothesis. Horsepower of 98 is associated with around a predicted 24.47 mpg, and 95% confidence interval is [23.97308, 24.96108] and 95% prediction interval is [14.8094, 34.12476]. There appears to be a somewhat strong relationship between the response and predictor since the R^2 statistic is 0.6049, meaning almost 2/3rds of the variance in horsepower is due to the linear regression on mpg. The RSS is 4.906, and given the mean mpg is 23.44, there is around a 21% error in prediction terms on average, indicative of a decent fit. \n",
    "\n",
    " While it is hard to say exactly what F-statistic cutoff we should use as a must for a strong rejection of null hypothesis especially since this is a small dataset of only 392 observations, the F-statistic is far greater than 1 so we can assume a significant relationship at this point. The prediction interval is significantly wider than the confidence interval (which usually is the case). The slope of the fitted line is around -0.15 (negative relationship) and the intercept is around 39.94. \n",
    "\n",
    " Here's the R code I used and summary statistics:\n",
    "\n",
    "SUMMARY STATISTICS:\n",
    "```{r}\n",
    "require(ISLR)\n",
    "Auto = ISLR::Auto\n",
    "lm_fit <- lm(mpg ~ horsepower, data = Auto)\n",
    "summary(lm_fit)\n",
    "```\n",
    "OUTPUT: \n",
    "Call:\n",
    "lm(formula = mpg ~ horsepower, data = Auto)\n",
    "\n",
    "Residuals:\n",
    "     Min       1Q   Median       3Q      Max \n",
    "-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\n",
    "horsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n",
    "\n",
    "Signif. codes:  0 ‘* * *’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 4.906 on 390 degrees of freedom\n",
    "Multiple R-squared:  0.6059,\tAdjusted R-squared:  0.6049 \n",
    "F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n",
    "\n",
    "\n",
    "CONFIDENCE AND PRED INTERVALS: \n",
    "```{r}\n",
    "to_predict = data.frame(horsepower = c(98))\n",
    "#confidence and prediction by default does 95% intervals, so I assume I don't need to specify interval bounds here?\n",
    "predict(lm_fit, newdata =  to_predict, interval = \"confidence\")\n",
    "predict(lm_fit, newdata =  to_predict, interval = \"prediction\")\n",
    "```\n",
    "OUTPUT:\n",
    "       fit      lwr      upr\n",
    "1 24.46708 23.97308 24.96108\n",
    "       fit     lwr      upr\n",
    "1 24.46708 14.8094 34.12476\n",
    "\n",
    "\n",
    "## (b)\n",
    "```{r}\n",
    "plot(horsepower, mpg)\n",
    "abline(lm_fit)\n",
    "```\n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "\n",
    "## (c)\n",
    "\n",
    "Linear regression may not be the most appropriate method of fitting the data, but it isn't unreasonable.\n",
    "The third figure (Scale-Location) has a conical pattern of standarzied residual error terms as you fit higher horsepower values to mpg, and this lack of heteroscadiscity violates an assumption of linear regression, so the model doesn't perfectly suit the conditions of ground truth data. The second graph (Q-Q Residuals) have the desired linear pattern between theoretical quantiles and standardized residuals and that means that horsepower and mpg follow very similar distributions. Despite this similarity between input and target variables, the first plot (Residuals vs Fitted) has only a slightly parabolic shape in the residual terms the model tries to fit; while it doesn't have the undesired horizontal residual fit line that centers at 0, it makes it clear that the data doesn't follow a strict linear relationship (how strict is hard to tell).In the fourth plot, all data points seem to be within the Cook's distance influence range and so the model's fit wasn't heavily skewed by any outliers (which is good for regression).\n",
    "\n",
    "Diagnostic plots below:\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "![Alt text](image-2.png)\n",
    " \n",
    "![Alt text](image-3.png)\n",
    "\n",
    "![Alt text](image-4.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.10 (p. 124-125 ISLR)\n",
    "\n",
    "## (a)\n",
    "\n",
    "CODE (R):\n",
    "```{r}\n",
    "multi_lr <- lm(Sales ~ Price + Urban + US, data = carseats)\n",
    "summary(multi_lr)\n",
    "```\n",
    "SUMMARY STATS:\n",
    "Call:\n",
    "lm(formula = Sales ~ Price + Urban + US, data = carseats)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-6.9206 -1.6220 -0.0564  1.5786  7.0581 \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 13.043469   0.651012  20.036  < 2e-16 ***\n",
    "\n",
    "Price       -0.054459   0.005242 -10.389  < 2e-16 ***\n",
    "\n",
    "UrbanYes    -0.021916   0.271650  -0.081    0.936 ***   \n",
    "\n",
    "USYes        1.200573   0.259042   4.635 4.86e-06 ***\n",
    "\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.472 on 396 degrees of freedom\n",
    "Multiple R-squared:  0.2393,\tAdjusted R-squared:  0.2335 \n",
    "F-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16\n",
    "\n",
    "## (c)\n",
    "\n",
    "The fitted equation can be stated as :\n",
    "sales = 13.043469  - 0.054459*Price  - 0.021916*UrbanYes + 1.200573*USYes\n",
    "\n",
    "## (b)\n",
    "\n",
    "Price coefficient: very small negative relationship to sales (small coefficient), but relationship is signfiicant since p-value is very small and t-value is rather \n",
    "\n",
    "UrbanYes Coefficient: very very small negative coefficient, but there appears to likely be no significant relationship to sales since p-value is rather high. (and t-statistic is small and matches distribution expected of null hypothesis) . Assume null hypothesis\n",
    "\n",
    "UsYes Coefficient: strong positive relationship (low p-value, t-value is high enough to indicate relationship significance )\n",
    "\n",
    "r-squared is just 0.2335, which is poor overall fit. Residual standard error is 2.472, which is significant given this is in thousands of units (so off  by around 2.5k units on average...a 34% error...)\n",
    "\n",
    "## (d)\n",
    "\n",
    "As discussed in part b above, price and USYes likely have a relationship to sales so I reject null for those two \n",
    "\n",
    "## (e)\n",
    "\n",
    "CODE\n",
    "```{r}\n",
    "two_lr <- lm(Sales ~ Price + US, data = carseats)\n",
    "summary(two_lr)\n",
    "```\n",
    "OUT\n",
    "Call:\n",
    "lm(formula = Sales ~ Price + US, data = carseats)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-6.9269 -1.6286 -0.0574  1.5766  7.0515 \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 13.03079    0.63098  20.652  < 2e-16 ***\n",
    "\n",
    "Price       -0.05448    0.00523 -10.416  < 2e-16 ***\n",
    "\n",
    "USYes        1.19964    0.25846   4.641 4.71e-06 ***\n",
    "\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.469 on 397 degrees of freedom\n",
    "Multiple R-squared:  0.2393,\tAdjusted R-squared:  0.2354 \n",
    "F-statistic: 62.43 on 2 and 397 DF,  p-value: < 2.2e-16\n",
    "\n",
    "R-squared is almost identical even with the fit only by the variables we thought significant\n",
    "\n",
    "## (f) \n",
    "\n",
    "There is no notable improvement in fitting sales on only to USYes and Price. At best, the r-squared value went up by a few hundredths, and the residual standard error went down too by a few hundredths, so there was a very minor improvement on fitting to only price and USYes. Otherwise, both models are mediocre. \n",
    "\n",
    "## (g) \n",
    "\n",
    "95% confidence intervals for price are [-0.06475984, -0.04419543] and USYes are [0.69151957,  1.70776632]\n",
    "\n",
    "## (h) \n",
    "\n",
    "Again, we see that in the Residuals vs Leverage plot (fourth figure), whlie a handful of points get close to Cook's distance, all of the standardized residuals have no significant leverage and saty within Cook's distance. The Residuals vs Fitted curve has residuals constrained to within 5 of the target, and the distribution of residuals seems to be relatively and randomly uniform around 0 (which is an undesired pattern for linear regression) and standardized residual distances fall within 1.5. (Scale-Location graph) There appears to be no significant outliers with much higher residuals than the rest of the data.\n",
    "\n",
    "\n",
    "DIAGNOSTIC PLOTS BELOW\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "![Alt text](image-7.png) \n",
    "\n",
    "![Alt text](image-8.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
